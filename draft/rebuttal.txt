POPL 2022 Paper #575 Reviews and Comments
===========================================================================
Paper #575 Symbolic Execution for Randomized Programs


Review #575A
===========================================================================

Overall merit
-------------
B. Weak Accept

Reviewer expertise
------------------
X. Expert

Paper summary
-------------
This paper shows that symbolic execution, which turns a program into a stream of execution paths paired with symbolic conditions on them, can be extended to probabilistic programs and used to verify and disprove bounds on discrete probabilities. The paper describes the probabilistic symbolic execution algorithm. The authors have implemented the algorithm on top of KLEE. The paper evaluates the implementation by applying it to concrete instances of several practically relevant properties of interesting probabilistic programs.

Strengths
---------
To my knowledge, this is the first time symbolic reasoning has been applied to probabilistic programs written in (or compiled to) a language as general-purpose and popular as LLVM. Unlike so-called "lightweight" inference methods, which run the program in a controlled environment, symbolic reasoning can handle (even universally quantify over) unknown inputs.

Weaknesses
----------
The paper is coy about the language of symbolic formulas generated by probabilistic symbolic execution and shipped off to an SMT solver. The size and form of this formula can greatly affect the efficacy of SMT solving, so reproducing the results in this paper depends on details about p in Algorithm 1 -- details as small as the $\cdot$ denoting symbolic multiplication on lines 27-28. In Algorithm 3 in particular, does line 4 assign to $p_c$ an expression that contains two summation _loops_, or do the $\sum$s indicate that the algorithm loops over values $v_1,\ldots,v_n$ to build large expressions that contain the binary operator $+$? Here "large" means enumerating the entire probability space!

The paper should be more helpful in contrasting the present work with related work.
- It is clear that "inputs that are unknown values" and "not drawn from a known distribution" are key distinctions of the present work, but the paper only presents the uniform distribution (L420), and does not say what is hard about, or how to deal with, unknown distributions. As for unknown inputs, the word "Most" (L1174, in contrast to "all", L1160) needs to come with details about the complement of "most".
- The "variety of approaches" (L1171) to probabilistic program inference is very large. It is inaccurate to describe Claret et al. 2013 as applying computer algebra systems, and even Gehr et al. roll their own computer algebra instead of applying an existing system. Which of these approaches and systems, if any, handle unknown inputs? Hakaru does [Carette & Shan 2016, Walia et al. 2019].
- Model checking's answer to large or infinite state spaces is abstraction. Without discussing abstraction and without empirical evaluation, it is not clear that "our examples...are far too large to encode in existing model checking tools" (L1191). It is also unclear that "our technique applies to infinite state systems", without worked-out examples and without details about SMT solving.
- It is unclear why conditioning would be "challenging" to handle (L1178). Why not just conjoin the predicate $\psi$ with the condition?

Comments for author
-------------------
On page 4, $=$ is used both in the metalanguage (to equate expressions) and in the object language (to equate numbers, like $\ne$ is used). This is confusing, especially on L150 and L187. Two different symbols should be used (perhaps `==` and `!=` in the object language), and parentheses should be used to clarify precedence.

L188-189 "this tree ... it does": But this single tree for car_door=1 does not tell us that "it is possible to win the car with any choice of initial door and decision to switch", because if (for example) the choice of initial door is 1 and the decision is to switch, then the car is not won. It takes multiple trees, one for each car_door value, to tell us that "it is possible ...".

L190-191 "how likely it was to take one branch over another": But in a tree where car_door is specified, there is no notion of "likely", so this sentence doesn't make sense.

L194 "universal symbolic variables": add "such as $\alpha$ and $\beta$"

L213 "from...from": superfluous repetition

L260 "jumps execution to statement T": I was expecting jumps to target labels, not statements. How are loops expressed in pWhile?

L267, L271: Given the explanation of $\mu$ on L268, it doesn't make sense for Prog as a function to take $\psi$ as second input, even if Algorithm 1 takes $\psi$ as input.

Algorithm 1:
- Line 2 and line 7 both bind $\sigma$, which is confusing as to whether there are two variables or one.
- On line 4, shouldn't the first $\emptyset$ be $\sigma$ instead?
- On line 7, $P,p$ should be shaded.
- Is it a problem that Algorithm 1 might not terminate when Prog contains a (terminating, even) loop?

L417 "is denotes"

L478 "line 3" -> "line 4"

L506.5 $e$ appears out of nowhere and is not explained.

L512-513 Even if "$\psi_{sym}$ is satisfiable", it might be too much to add all of $p$ to $Enc_\psi$. For example, suppose Prog is `x ~ uniform_int(1,3); halt` and $\psi$ is `x=1`. This Prog does not contain `if`, so Algorithm 1 only has one path to follow, and $p$ stays at $1.0$. Yet the probability that $\psi$ holds is not 1 but 1/3. It seems that Algorithm 1 line 34 and Algorithm 4 line 2 should take $\psi_{sym}$ into account.

L519-520: What are $p_1$ and $p_2$? Also, parentheses seem missing before "/3"

L522: To clarify "for all", add "total" before "probability". Also here, defer explicitly explaining Algorithm 1 line 36.

L527 "not complete": This important fact should not be buried here. Also, give an example.

L529 "some missing paths might violate the lower bound" is confusing because paths do not violate bounds. Reword: perhaps "missing some paths might give the false appearance that the program violates a lower bound"

L545.5 "With can" -> "We can"

L548.5: Is $\psi := \beta \land \mathsf{win}$ really the predicate we're interested in? If $\beta$ is false, then the probability of $\psi$ is 0, not 2/3 as claimed later in this paragraph!

L753: But if $A[1]=A[2]$ then $\psi$ has probability $0$, not $k/n$ as required by the query!

L792: But if $\mathsf{DistToMono}(f)\ne k$ then $\psi$ has probability $0$, not $>k/n$ as required by the query! Also, the query is missing $\forall k$

L822 $\mathbb{E}[\texttt{num\_comps}]$: What is the distribution that this expected value is computed over? Is each element of $A$ chosen independently from the same uniform distribution? What is the query whose form is described on L570?

Algorithm 10 line 11 should use $\sim$ rather than $\leftarrow$

Don't use both $\epsilon$ and $\varepsilon$ (page 19).

L927: Remind the reader that $e$ means the base of the natural logarithm.

L929 "updated" -> "incremented" to be more specific

Algorithm 12 line 21 uses $n$, which is inconsistent with $N$ on L961, L962

L968: Clarify that this predicate $\psi$ only tests the special case where $a_{X[1]} = 1$

Section 5 should clarify up front what is "verification" (L1005, L1140), what is "bug finding" (L1010), and which of these is achieved in each evaluated example. This is especially confusing in the discussion of (4) on page 23.
- It has to be acknowledged that using AllOff or FirstOff may leave some bugs not found. For example, using AllOff or FirstOff in $\psi$ would not test Freivalds' Algorithm for $A,B,C$ such that $(A\times B)_{0,0}=C_{0,0}$ but $(A\times B)_{2,3}\ne C_{2,3}$. Thus, any "increased performance" may come at the cost of no longer performing "verification" or even "bug finding".
- What is the difference between SomeOff and OneOff? What does "we represent $i$ and $j$ as universal symbolic variables and universally quantify them in Equation (2)" mean? Give the actual $\psi$ used. Why does SomeOff provide a stronger guarantee than OneOff (L1116)?
- Some indices in (4) start at 0, others at 1.

L997 ", for" -> " for,"

L1021 "experiments a" -> "experiments on a"

L1070 "this suggests that constraint solving is the main bottleneck in our approach, not path exploration": The concretizations (Table 1 rightmost column) are pretty small, and that may be why path exploration is not yet a bottleneck.

In table columns, align numbers to the right to ease comparison.

Number all displayed equations.

Spell check.



Review #575B
===========================================================================

Overall merit
-------------
B. Weak Accept

Reviewer expertise
------------------
Y. Knowledgeable

Paper summary
-------------
Symbolic execution is a well-known technique for exploring programs. This work contributes the ability to combine universally quantified input variables with randomly drawn stochastic variables inside the program. Existing techniques that account for probabilities usually do not allow input variables or assume that they are drawn from a uniform distribution.

The standard symbolic execution algorithm is extended to keep track of probability distributions, as well as the probability associated with the current branch being explored. This information can be collected and summed to calculate the probability of a particular property holding or being violated. A base language and semantics is given, and the algorithm is shown to calculate the probabilities correctly. An implementation is provided and a number of case studies show that several small real-world algorithms can be analysed.

Strengths
---------
Probabilities can be useful when modelling situations where the expected outcome is more important than pathological cases. Similarly, many random algorithms achieve speed ups over non-random algorithms at the cost of some possibility of error. Being able to analyse these programs to confirm that the probabilities of outcomes match the guarantees claimed is a welcome extension to the symbolic execution framework.

Although only a small number of case studies are considered, they have been manually curated, and at least show the feasibility of the work.

The work is a natural extension to the standard algorithm, and careful proofs appear to have been provided, though I would have preferred a nicer presentation of the formal work in the main body.

Weaknesses
----------
I did not dig through the supplementary material, but the intuition behind the approach is natural, and the authors did not claim any particular challenges that needed to be overcome.

The experiments cover only a small number of benchmarks, and notably do not show how the algorithm would cope with a program that is large enough to prevent a full path exploration completing. In some sense, all benchmarks were verifiable. It is claimed in the related work that existing probabilistic model checkers would not be able to cope with the state-spaces of the benchmarks. I would like to see more evidence for this. E.g. PRISM uses a BDD data structure that can potentially handle large state spaces.

The experiments only really answer Q2 and Q3 for a specific benchmark. Is there anything we can generalise from this?

Only a naive version of symbolic execution is considered, though all work must start somewhere.

Comments for author
-------------------
I did not find the formal treatment in Section 3.4 to be illuminating. I had the feeling that there were too many latin/greek letters that were hard to keep track of, several appearing to take on similar roles, with no overarching sense of purpose.

Theorem 3.2 is presented rather oddly -- it shows that p matches the semantics, but is presented as if the semantics needs to justify not tracking p when the algorithm does! More generally, the section is presented as a formalisation of Algorithm 1's behaviour. I think it would be more natural to define the pWhile language, then the semantics of the pWhile language and what the probabilities should be independently of the Algorithm, then show that Algorithm 1 respects these semantics. I believe this is what has been done, but the presentation is a little back-to-front.

Theorems 3.3-3.5 show individual steps towards an overall statement of correctness that is not given.

It is odd to me that the definition of "μ satisfies our abstraction R" does not seem to care about the probabilities calculated, only that they are greater than 0, yet somehow, for the entire algorithm to be correct, the precise probabilities should matter. Precise probabilities are only treated in Theorem 3.5. Possibly Theorem 3.2 shows that things are indeed tracked correctly?

Is there a typo somewhere in the definition of "μ satisfies our abstraction R"? I could not see where μ occurs in Line 624. The "and if" on Line 624 also seems to imply that μ can only satisfy R if the definition of ν_φ somehow succeeds.

μ seems to be key to the semantics and proof, but it is not really explained or motivated.

The use of the Pr[...] notation causes a bit of confusion with the Iverson brackets [...].

Section 4 takes up a lot of space, but I do not find it as interesting as I would a cleaner presentation of the formalisation. The details of the case study algorithms are not crucial to the results and can be looked up elsewhere if needed.



Review #575C
===========================================================================

Overall merit
-------------
C. Weak Reject

Reviewer expertise
------------------
Y. Knowledgeable

Paper summary
-------------

This paper presents a symbolic execution algorithm that can 
handle both traditional inputs and variables drawn 
from a probability distribution.

Strengths
---------
Pros:
+ Intuitive algorithm with proof of correctness.
+ Well explained.

Weaknesses
----------
Cons:
- The proposed algorithm for handling (potentially probabilistic)
branches is essentially exhaustive, which means that the domains 
of probabilistic variables must be very small for the algorithm 
to terminate. This is evident from the evaluation where the concretization
limits for Freivalds, Quicksort, Bloom filters, etc are in the low 
single digits.
- Perhaps the most interesting application for this work would be probabilistic 
programming inference. Unfortunately, it doesn't handle conditional
probabilities, and it doesn't seem like there is a reasonable way to 
extend the algorithm to handle them.

Comments for author
-------------------
Minor:

- With respect to the Bloom filter case study, it might be interesting to not only
try and find the bug in the original algorithm, but also the known 
proposed fix (which was left undiscovered for quite sometime). See Ilya Sergey's
and Kiran Gopinathan's work in CAV'20 for more.

- l 1069: analyzine

Questions for the response period
---------------------------------
Question:
- While you do state that you made little effort to optimize your symbolic
execution, the "optimization" direction of future work seems to convey that 
you believe it's just a matter of developing heuristics. While I whole-heartedly agree 
that a solid foundation, like the one presented, should be at the core of 
any reasonable symbolic executor, can you give any intuition as to why you 
expect your approach to scale, when the computation of path probabilities 
and solving them with Z3 seems to already dominate the execution time?
